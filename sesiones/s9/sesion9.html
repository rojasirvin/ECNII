<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modelos de selección</title>
    <meta charset="utf-8" />
    <meta name="author" content="Irvin Rojas" />
    <script src="libs/header-attrs-2.3/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="libs\cide.css" type="text/css" />
    <link rel="stylesheet" href="https:\\stackpath.bootstrapcdn.com\bootstrap\4.3.1\css\bootstrap-grid.min.css" type="text/css" />
    <link rel="stylesheet" href="https:\\use.fontawesome.com\releases\v5.7.2\css\all.css" type="text/css" />
    <link rel="stylesheet" href="https:\\cdn.rawgit.com\jpswalsh\academicons\master\css\academicons.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: title-slide



&lt;style type="text/css"&gt;
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}
&lt;/style&gt;


.title[
# Sesión 9. Modelos de selección
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas &lt;br&gt; [rojasirvin.com](https://www.rojasirvin.com/) &lt;br&gt; [&lt;i class="fab fa-github"&gt;&lt;/i&gt;](https://github.com/rojasirvin) [&lt;i class="fab fa-twitter"&gt;&lt;/i&gt;](https://twitter.com/RojasIrvin) [&lt;i class="ai ai-google-scholar"&gt;&lt;/i&gt;](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&amp;hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas &lt;br&gt; División de Economía
]

---
# Agenda
  
1. Introduciremos una primera forma de manejar la selección

1. Estudiaremos la selección en los datos con dos posibles orígenes:

  - Comportamiento 

  - Codificación o
  
1. Podemos arreglarlo modelando explícitamente el proceso de selección

---

class: inverse, middle, center

# Censura y truncamiento

---

# Mecanismos de censura y truncamiento

- Consideremos una variable latente `\(y^*\)` que se observa de acuerdo a una regla de observación `\(g(\cdot)\)`

- Lo que observamos es `\(y=g(y^*)\)`

---

# Censura

- Siempre observamos `\(X\)` pero no `\(y\)`:

  - Censura por abajo: `\(y=\begin{cases}y^* \quad \text{si }y^*&gt;L \\ L \quad \text{si }y^*\leq L \end{cases}\)`
  
  - Censura por arriba: `\(y=\begin{cases}y^* \quad \text{si }y^*&lt;U \\ U \quad \text{si }y^*\geq L \end{cases}\)`
  
- El típico ejemplo de censura se encuentra en los datos *top coded*, como los de ingreso

---

# Truncamiento

- Tanto `\(X\)` como `\(y\)` son no observados para ciertos valores de `\(y\)`

  - Truncamiento por abajo: `\(y=y^*\)` si `\(y^*&gt;L\)` y no osbervada si `\(y^*\leq L\)`
  
  - Truncamiento por abajo: `\(y=y^*\)` si `\(y^*&gt;U\)` y no osbervada si `\(y^*\geq U\)`

---

# Función de verosimilitud censurada

- La censura y el truncamiento cambian la función de verosimilitud de los datos observados


- Verosimilitud censurada (usando censura por abajo)

- Cuando `\(&gt;L\)`, la densidad de `\(y\)` es la misma que la de `\(y^*\)`, es decir, `\(f(y|x)=f^*(y|x)\)`

- Cuando `\(y=L\)`, la densidad es discreta con masa igual a la probabilidad de que `\(y^*\leq L\)`

- En resumen
$$
f(y|x)=
`\begin{cases}
f^*(y|x) \quad\text{si } y&gt;L \\
F^*(L|x)\quad\text{si }y=L \\
\end{cases}`
$$
--

- La densidad es un híbrido entre una función de masa de probabilidad (una densidad propiamente) y una función de densidad acumulada

---

# Función de verosimilitud censurada

- Definamos

$$
d=
`\begin{cases}
1\quad\text{si }y&gt;L \\
0\quad\text{si }y=L \\
\end{cases}`
$$

- Entonces la densidad condicional debido a la censura es:

`$$f(y|x)=f^*(y|x)^dF^*(L|x)^{1-d}$$`

--

- Y la función de log verosimilitud será
`$$\mathcal{L}_N{\theta}=\sum_i\left(d_i\ln f^*(y_i|x_i,\theta) + (1-d_i)F^*(L_i|x_i,\theta)\right)$$`

- Noten que hemos dejado abierta la opción de que `\(L\)` difiera entre individuos, es decir, que `\(L=L_i\)`

- Si la densidad de `\(y^*\)`, `\(f^*(y^*|x,\theta)\)`, está bien especificada, `\(\theta_{MV}\)` es consitente y asintóticamente normal

---

# Función de verosimilitud truncada

- Consideremos el caso de truncamiento por abajo

- Noten que la función de densidad de `\(y\)` es:

$$
`\begin{aligned}
f(y)&amp;=f^*(y|y&gt;L) \\
&amp;=\frac{f^*(y)}{P(y|y&gt;L)}\\
&amp;=\frac{f^*(y)}{1-F^*(L)}
\end{aligned}`
$$
--

- Entonces, la log verosimilitud truncada es:

`$$\mathcal{L}_N{\theta}=\sum_i\left(\ln f^*(y_i|x_i,\theta)-\ln(1-F^*(L_i|x_i,\theta))\right)$$`

---

class: inverse, middle, center

# Modelo Tobit

---

# Modelo Tobit

- Es una modelo simple y con supuestos muy fuertes sobre la estructura de la censura

- Tobin (1958) lo planteó originalmente como una forma de modelar la compra de bienes durables (muchos hogares gastan 0 en bienes durables)

--

- Consideramos un proceso con errores normales
$$
`\begin{aligned}
&amp;y^*=x'\beta+\varepsilon \\
&amp;\varepsilon\sim\mathcal{N}(0,\sigma^2)
\end{aligned}`
$$
- Supongamos que observamos `\(y\)` de acuerdo a la siguiente regla:

$$
y=
`\begin{cases}
y^*\quad\text{si }y^*&gt;0 \\
-\quad\text{si } y^*\leq 0\\
\end{cases}`
$$
---

# Modelo Tobit

- Con los errores normales, podemos definir la cdf como
$$
`\begin{aligned}
F^*(0)&amp;=P(y^*\leq0) \\
&amp;=P(x'\beta+\varepsilon\leq 0) \\
&amp;=\Phi(-x'\beta/\sigma) \\
&amp;=1-\Phi(x'\beta/\sigma)
\end{aligned}`
$$
--

- Esto nos permite definir la densidad censurada como
$$
f(y)=\left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(-\frac{1}{2\sigma^2}(y-x'\beta)^2\right)\right)^d\left(1-\Phi\left(\frac{x'\beta}{\sigma}\right)\right)^{1-d}
$$
--

- Y entonces la función de log verosimilitud será:
$$
`\begin{aligned}
\mathcal{L}_N(\beta,\sigma^2)&amp;=\sum_i\left( d_i\left(-\frac{1}{2}\ln(\sigma^2)-\frac{1}{2}\ln(2\pi)-\frac{1}{2\sigma^2}(y_i-x'\beta)^2\right)+ \right. \\
&amp;\left. +(1-d_i)\ln\left(1-\Phi\left(\frac{x_i'\beta}{\sigma}\right)\right) \right)
\end{aligned}`
$$

---

# Condiciones de primer orden

`$$\frac{\partial \mathcal{L}_N}{\partial \beta}=\sum_i\frac{1}{\sigma^2}\left(d_i(y_i-x_i'\beta)-(1-d_i)\frac{\sigma \phi_i}{1-\Phi_i}\right)x_i=0$$`
`$$\frac{\partial \mathcal{L}_N}{\partial \sigma^2}=\sum_i\left(di\left(-\frac{1}{2\sigma^2}+\frac{(y_i-x_i'\beta)^2}{2\sigma^4}\right)+(1-d_i)\left(\frac{\phi_ix_i'\beta}{(1-\Phi_i)2\sigma ^3}\right)\right)=0$$`
--

- La solución se obtiene numéricamente

- `\(\hat{\theta}\)` es consistente si la densidad está bien especificada

- El estimador de MV es asintóticamente normal: `\(\theta\stackrel{a}{\sim}\mathcal{N}(\theta,V(\hat{\theta})\)`

- Maddala (1983) y Amemiya (1985) proveen expresiones para la matriz de varianzas

---

# Nota sobre terminología

- El modelo Tobit fue plantado inicialmente para un problema de censura en cero

- Cuando nos refiramos al Tobit estaremos pensando en la estructura particular que tienen `\(y^*\)` y `\(y\)`

- Si en vez de censura, ocurriera truncamiento, la log verosimilitud sería
`$$\mathcal{L}_N(\beta,\sigma^2)=\sum_i \left(-\frac{1}{2}\ln(\sigma^2)-\frac{1}{2}\ln(2\pi)-\frac{1}{2\sigma^2}(y_i-x'\beta)^2-\ln\left(\Phi(x_i'\beta/\sigma)\right)\right)$$`
---

# ¿Cuáles son las consecuencias de la censura y el truncamiento?

- Para ver las consecuencias de ignorar el truncamiento o la censura, consideremos el modelo lineal de variable latente 

`$$E(y ^*|x)=x'\beta$$`


- Consideremos la medica condicional censurada en cero

- Calculamos `\(E(y)\)`:
$$
`\begin{aligned}
E(y)&amp;= P(d = 0)E(y|d = 0) + P(d = 1)  E(y|d = 1) \\
&amp;= 0 P(y∗ ≤ 0) + P(y∗&gt; 0) E(y∗|y∗&gt;0) \\
&amp;= P(y∗&gt; 0)  E(y∗|y∗&gt; 0)
\end{aligned}`
$$
--

Por tanto:

`$$E(y|x)=P(\varepsilon&gt;-x'\beta)(x'\beta+E(\varepsilon|\varepsilon&gt;-x'\beta))$$`
---

# ¿Cuáles son las consecuencias de la censura y el truncamiento?

- Veamos ahora la media condicional truncada en cero

$$
`\begin{aligned}
E(y)&amp;=E(y^*|y^*&gt;0) \\
&amp;=E(x'\beta+\varepsilon|x'\beta+\varepsilon&gt;0) \\
&amp;=E(x'\beta|x'\beta+\varepsilon&gt;0)+E(\varepsilon|x'\beta+\varepsilon&gt;0) \\
\end{aligned}`
$$
- Por tanto

--

`$$E(y|x,y&gt;0)=x'\beta+E(\varepsilon|\varepsilon&gt;-x'\beta)$$`
--

- Aunque la media condicional de `\(y^*\)` es lineal, la media condicional de `\(y^*\)` no lo es

- El estimador de MCO es inconsistente en presencia de censura o truncamiento

- Notemos que una pieza clave para entender estos valores esperados es `\(E(\varepsilon|\varepsilon&gt;-x'\beta)&gt;0\)`

---

# Momentos truncados de la distribución normal

- Si `\(z\sim\mathcal{N}(0,1)\)`, la esperanza y varianza truncadas de la distribución truncada tienen las siguientes formas:

1. `\(E(z|z&gt;c)=\frac{\phi(c)}{1-\Phi(c)}\)` y `\(E(z|z&gt;-c)=\frac{\phi(c)}{\Phi(c)}\)`

1. `\(E(z^2|z&gt;c)=1+\frac{c\phi(c)}{1-\Phi(c)}\)`

1. `\(V(z|z&gt;c)=1+\frac{c\phi(c)}{1-\Phi(c)}-\frac{\phi(c)^2}{(1-\Phi(c))^2}\)`

--

- Comúnmente a `\(\lambda(z)=\frac{\phi(z)}{\Phi{z}}\)` se le conoce como **inverso de la razón de Mills**


---

# Medias condicionales en el modelo Tobit

.pull-left[
- Podemos entonces obtener una expresión para `\(E(\varepsilon|\varepsilon&gt;-x'\beta)\)`:

$$
`\begin{aligned}
E(\varepsilon|\varepsilon&gt;-x'\beta)&amp;=\sigma E\left(\varepsilon/\sigma|\varepsilon/\sigma&gt;-x'\beta/\sigma\right) \\
&amp;=\sigma \left(\frac{\phi(-x'\beta/\sigma)}{1-\Phi(-x'\beta/\sigma)}\right) \\
&amp;=\sigma\frac{\phi(x'\beta/\sigma)}{\Phi(x'\beta/\sigma)}\\
&amp;=\sigma\lambda(x'\beta/\sigma)
\end{aligned}`
$$
- Finalmente, noten que:
$$
`\begin{aligned}
P(\varepsilon&gt;-x'\beta)&amp;=P(-\varepsilon&lt;x'\beta) \\
&amp;=P(-\varepsilon/\sigma&lt;x'\beta/\sigma)\\
&amp;=\Phi(x'\beta/\sigma)
\end{aligned}`
$$
]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/mills_ratio.png" alt="Fuente: Cameron &amp;amp; Trivedi (2005)" width="120%" /&gt;
&lt;p class="caption"&gt;Fuente: Cameron &amp; Trivedi (2005)&lt;/p&gt;
&lt;/div&gt;

- Noten que `\(\lambda(z)\)` es casi lineal en `\(c\)` para valores de `\(c&gt;0\)`
]

---

# Media y varianza condicional

- Los resultados anteriores nos permiten obtener expresiones para la media y varianza condicional de la distribución truncada **cuando `\(\varepsilon\)` es normal**:

.pull-left[
- Truncada en cero
$$
`\begin{aligned}
&amp;E(y|x,y&gt;0)=x'\beta+\sigma\lambda(x'\beta/\sigma) \\
&amp;V(y|x,y&gt;0)=\sigma^2(1-x'\beta\lambda(x'\beta/\sigma)-\lambda(x'\beta/\sigma)^2)
\end{aligned}`
$$

- Censurada en cero
$$
`\begin{aligned}
&amp;E(y|x)=\Phi(x'\beta/\sigma)x'\beta+\sigma\phi(x'\beta/\sigma)\\
&amp;V(y|x)=\sigma^2\Phi(x'\beta/\sigma)\left((x'\beta)^2+x'\beta\lambda(x'\beta)+1- \\ -\Phi(x'\beta/\sigma)(x'\beta+\lambda(x'\beta/\sigma))\right)
\end{aligned}`
$$

]

.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/cond_means.png" alt="Fuente: Cameron &amp;amp; Trivedi (2005)" width="120%" /&gt;
&lt;p class="caption"&gt;Fuente: Cameron &amp; Trivedi (2005)&lt;/p&gt;
&lt;/div&gt;
]

---

# Efectos marginales

- Los resultados anteriores nos permiten derivar los efectos marginales para la variable latente `\(y^*\)` y para la variable observada `\(y\)`

- Efecto marginal en la variable latente: `\(\frac{\partial E(y^*)}{\partial x}=\beta\)`

- Efecto marginal en la media truncada en cero: `\(\frac{\partial E(y,y&gt;0|x)}{\partial x}=(1-x'\beta\lambda(x'\beta/\sigma)-\lambda(x'\beta/\sigma)^2)\beta\)`

- Efecto marginal en la media censurada en cero:
`\(\frac{\partial E(y|x)}{\partial x}=\Phi(x'\beta/\sigma)\beta\)`

--

- ¿cuál efecto nos interesa? Depende de la pregunta

- A veces la censura o el truncamiento ocurren simplemente por una cuestión de recolección o codificación de datos, así que el objeto de interés sigue siendo `\(\frac{\partial E(y^*)}{\partial x}\)`

- Otras veces, la variable censurada o truncada tiene una interpretación de comportamiento económico
  
  - Horas trabajadas deseadas `\(y^*\)` vs horas trabajadas observadas `\(y\)`
  
---

# Conclusión triste sobre el Tobit

- Relativamente simple y con resultados que pueden obtenerse de forma directa

--

- Recae en una especificación altamente restrictiva de errores normales y homocedásticos

- Poco usado en la práctica

- Es útil porque nos permitió estudiar las consecuencias de la censura y el truncamiento en un contexto muy simplificado

- Es de esperarse que estas consecuencias se sostengan y vuelvan incluso más complejo el análisis cuando `\(y^*\)` y `\(y\)` tienen estructuras más generales


---

class: inverse, middle, center

# Modelos en dos partes

---

# Modelos en dos partes

- Modelamos dos procesos diferentes:

  - Participación: `\(d=\begin{cases} 1 \quad\text{si participa} \\ 0\quad\text{si no participa} \end{cases}\)`
  
  - Resultado: `\(y=\begin{cases} y \quad\text{si participa} \\ 0\quad\text{si no participa} \end{cases}\)`
  
--

- El modelo en dos partes está dado por:

$$
f(y |x)=
`\begin{cases}
P(d=0|x)\quad\text{si }y=0 \\
P(d=1|x)f(y|d=1,x)\quad\text{si }y&gt;0\\
\end{cases}`
$$
---

# Ejemplo 1: modelo en dos partes Poisson

- Se relajan los supuestos de que los ceros y los positivos son generados por el mismo proceso generador de datos

- Los ceros se determinan por la densidad `\(f_i(\cdot)\)`, por lo que `\(P(y=0)=f_1(0)\)`

- Los conteos positivos están dados por `\(f_2(y|y&gt;0)=\frac{f_2(y)}{1-f_2(0)}\)`, que se multiplica por `\(P(y&gt;0)=1-f_1(0)\)` para asegurar que las probabilidades sumen 1

--

- En este caso, el modelo en dos partes es:

$$
g(y)=
`\begin{cases}
f_1(0) \quad\text{si }y=0 \\
\frac{(1-f_1(0))f_2(y)}{1-f_2(0)}\quad\text{si }y\geq 1 
\end{cases}`
$$
- Una forma natural de especificar `\(f_2(\cdot)\)` es con un proceso Poisson o negativo binomial

- El modelo se estima por máxima verosimilitud

---

# Ejemplo 1: modelo en dos partes Poisson

- Este modelo se puede pensar como

  1. Lanzamos una moneda no balanceada y determinamos si `\(y=0\)` o si `\(y&gt;0\)`
  
  2. Si `\(y&gt;0\)`, entonces el conte Poisson sea realiza
  

- No necesitamos generar una densidad mezclada pues la densidad que resulta es un producto del proceso binomial y de conteo, por lo que el log de este producto es una suma

---

class: inverse, middle, center

# Modelos de muestras seleccionadas

---


# Modelos de muestras seleccionadas

- Las muestras pueden estar seleccionadas

  - Por el econometrista
  
  - Por que los agentes escogen participar
  
--

- Ecuación de participación

$$
y_1=
`\begin{cases}
1\quad\text{si } y_1^*&gt;0\\
0\quad\text{si } y_1^*\leq 0
\end{cases}`
$$
- Ecuación de resultados

$$
y_2=
`\begin{cases}
y_2^*\quad\text{si } y_1^*&gt;0\\
NA \quad\text{si } y_1^*\leq 0
\end{cases}`
$$
- Por tanto

$$
`\begin{aligned}
y_1^*=x_1'\beta_1+\varepsilon_1 \\
y_2^*=x_2'\beta_2+\varepsilon_2 \\
\end{aligned}`
$$
- En el caso en que `\(y_1^*=y_2^*\)`, el modelo se colapsa al Tobit


---

# Modelo de Heckman

- No hay consenso de cómo llamarlo

- El estimador que veremos fue desarrollado por Heckman

- Algunos otros autores le llaman **Tobit de Tipo II** o **modelo con ecuación de selección**

- Supuesto: errores con distribución conjunta normal

$$
\begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \end{pmatrix}\sim \mathcal{N}\left(A, B \right)
$$
- `\(A=\begin{pmatrix} 0 \\ 0  \end{pmatrix}\)`

- `\(B=\begin{pmatrix} 1 &amp; \sigma_{12} \\ \sigma_{21} &amp; \sigma_2^2 \\ \end{pmatrix}\)`

- `\(\sigma_1^2=1\)` es una normalización


---

# Resultados de la distribución normal conjunta

- Por nuestro supuesto de normalidad resulta que: `\(\varepsilon_2=\sigma_{12}\varepsilon_1 +\xi\)`

- `\(\xi\)` es independiente de `\(\varepsilon_1\)`

--

- **Media truncada**

`$$E(y_2|x,y_1^*&gt;0)=x_2'\beta_2+E(\varepsilon_2|\varepsilon_1&gt;-x_1'\beta_1)$$`

- Usando el resultado de la normalidad de los errores:

$$
`\begin{aligned}
E(y_2|x,y_1^*&gt;0)&amp;=x_2'\beta_2+E(\sigma_{12}\varepsilon_1+\xi|\varepsilon_1&gt;-x_1'\beta_1) \\
&amp;=x_2'\beta_2+\sigma_{12}E(\varepsilon_1|\varepsilon&gt;-x_1'\beta_1) \\
&amp;=x_2'\beta_2+\sigma_{12}\lambda(x_1'\beta_1)
\end{aligned}`
$$
- Similarmente
`$$V(y_2|x,y_1^*)=\sigma_2^2-\sigma_{12}^2\lambda(x_1'\beta_1)(x_1'\beta_1+\lambda(x_1'\beta_1))$$`
---

# Resultados de la distribución normal conjunta

- **Media censurada**

- Cuando `\(y_2=0\)` si `\(y_1^*&lt;0\)`

$$
`\begin{aligned}
E(y_2|x)&amp;=E_{y_{1}^{*}}(E(y_2|x,y_1^*)) \\
&amp;=P(y_1^*\leq 0 | x)\times 0+P(y_1^* &gt; 0 | x)E(y_2^*|X,y_1^*&gt;0) \\
&amp;=0+\Phi(x_1'\beta_1)(x_2'\beta_2+\sigma_{12}\lambda(x_1'\beta_1)) \\
&amp;=\Phi(x_1'\beta_1)x_2'\beta_2 + \sigma_{12}\phi(x_1'\beta_1)
\end{aligned}`
$$


---
# Próxima sesión

- Concluiré la discusión sobre el estimador de Heckman


- Veremos dos ejemplos aplicados:

  - **Tobit**: Zou, B., &amp; Luo, B. (2019). Rural Household Energy Consumption Characteristics and Determinants in China. Energy.
  - **Heckman**: Parey, M., Ruhose, J., Waldinger, F., &amp; Netz, N. (2017). The selection of high-skilled emigrants. *Review of Economics and Statistics*, 99(5), 776-792.

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script src="libs/cols_macro.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": null,
"scroll": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
